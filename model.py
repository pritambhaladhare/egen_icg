# -*- coding: utf-8 -*-
"""Copy of icg_keras

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZhTJcMw-HqDfyu2u019Hw0JaCrHyAwxR
"""

!unzip -q /content/drive/My\ Drive/dataset/img_txt -d /content/drive/My\ Drive/dataset/text
!unzip -q /content/drive/My\ Drive/dataset/imgs -d /content/drive/My\ Drive/dataset/images
!unzip -q /content/drive/My\ Drive/dataset/glove -d /content/drive/My\ Drive/dataset/text/Glove\ vectors

!wget -O /content/drive/My\ Drive/dataset/imgs "https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"

# Text Data
!wget -O /content/drive/My\ Drive/dataset/img_txt "https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"

# # Glove Vectors
!wget -O /content/drive/My\ Drive/dataset/glove "http://nlp.stanford.edu/data/glove.6B.zip"

!ls /content/drive/My\ Drive/dataset/glove

!mkdir dataset
!mkdir dataset/text
!mkdir dataset/images

# Image data
!wget -O /content/drive/My\ Drive/dataset/imgs "https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"

# Text Data
!wget -O dataset/img_txt "https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"

# Glove Vectors
!wget -O dataset/glove "http://nlp.stanford.edu/data/glove.6B.zip"


!unzip -q dataset/img_txt -d dataset/text
!unzip -q dataset/imgs -d dataset/images
!unzip -q dataset/glove -d dataset/text/Glove\ vectors


# !rm dataset/img_txt dataset/imgs

!cp -r dataset/ drive/My\ Drive/

!cd drive/My\ Drive/

!ls drive/My\ Drive/dataset/images/

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline
import pandas as pd
import string
from os import listdir
from pickle import dump, load
# from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.applications import InceptionV3
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
# from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg19 import preprocess_input

from numpy import array, argmax, asarray
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
from keras.preprocessing.sequence import pad_sequences
from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import GRU
from keras.layers import Embedding
from keras.layers import Dropout
from keras.layers.merge import add, concatenate
from keras.callbacks import ModelCheckpoint
from keras.models import load_model
from nltk.translate.bleu_score import corpus_bleu
import time

from google.colab import drive
drive.mount('/content/drive')

"""
  
  Input  :
    filename
  Output :
    pd.reader file
"""
 
def disp_img_cap_data(filename):
  reader = pd.read_csv(filename, header=None, delimiter=r"\t", names=['image_id','caption'])
  print(reader)
#   return reader

directory = "/content/drive/My Drive/dataset/images/Flicker8k_Dataset/"
filename_train = "/content/drive/My Drive/dataset/text/Flickr_8k.trainImages.txt"
filename_test  = "/content/drive/My Drive/dataset/text/Flickr_8k.testImages.txt"
filename_val   = "/content/drive/My Drive/dataset/text/Flickr_8k.devImages.txt"
filename = "/content/drive/My Drive/dataset/text/Flickr8k.token.txt"


project_dir = "/content/drive/My Drive/project/"

disp_img_cap_data(filename)

"""
  Input  : 
    filename of pic to image captions
  Output :
    pic_cap_dict - {pic_id} = [cap1,cap2,...,cap5]
"""
def create_pic_cap_dict(filename):
  
  reader = pd.read_csv(filename, header=None, delimiter=r"\t", names=['image_id','caption'])
  pic_cap_dict = dict()
  
  for indx,rows in reader.iterrows():
    id = rows['image_id'].split('.')[0]
    if id in pic_cap_dict:
      pic_cap_dict[id].append(rows['caption'])
    else:
      pic_cap_dict[id] = list()
  return pic_cap_dict


# For all images 
orig_pic_cap_dict = create_pic_cap_dict(filename)
pic_cap_dict = create_pic_cap_dict(filename)

# For train images 
train_set = {i for i,j in create_pic_cap_dict(filename_train).items()}
train_list = list(train_set)

# For test images 
test_set = {i for i,j in create_pic_cap_dict(filename_test).items()}
test_list = list(test_set)

# For validation 
val_set = {i for i,j in create_pic_cap_dict(filename_val).items()}
val_list = list(val_set)

print(len(val_list))
print(len(train_list))
print(len(test_list))

"""dataset/text/Flickr_8k.trainImages.txt
  Clean the database
  Input :
    pic_cap_dict
  Output :
    clean_pic_cap_dict
      Without punctuations
      all to lower case
      remove 's' and 'a'
      remove numbers
"""

def clean_db(pic_cap_dict):
  table = str.maketrans('','', string.punctuation)
  temp = pic_cap_dict
  for key, captions in pic_cap_dict.items():
    new_list = []

    # img=mpimg.imread(directory+''+key+'.jpg')
    # imgplot = plt.imshow(img)
    # plt.show()
    # print(key)

    for caption in captions:
      cap_to_words = caption.split()

    #########################
    #   print("Orignal Caption")
    #   print(cap_to_words)
    #   input()
    #########################

    #   Make the words lower case
      cap_to_words = [word.lower() for word in cap_to_words]

    #########################
    #   print("Lower Case")
    #   print(cap_to_words)
    #   input()
    #########################
    
      # Remove the captions
      cap_to_words = [word.translate(table) for word in cap_to_words]
    
    #########################  
    #   print("Removal of puctuations")
    #   print(cap_to_words)
    #   input()
    #########################
    
      # Remove the single 'a' and 's'
      cap_to_words = [word for word in cap_to_words if len(word) > 1]

      # Remove the words with numbers in them
      cap_to_words = [word for word in cap_to_words if word.isalpha()]
      cap_to_words.insert(0,"startseq")
      cap_to_words.append("endseq")
      caption = " ".join(cap_to_words)
    
    #########################
    #   print("Adding startseq and endseq tokens")
    #   print(cap_to_words)
    #   input()
    #########################

      new_list.append(caption)
    pic_cap_dict[key] = new_list
  return pic_cap_dict

# for i,c in temp.items():
#   print(c)


pic_cap_dict = clean_db(pic_cap_dict)

# Function to Get the most used words to form a dictionary
def get_vocab(pic_cap_dict, word_count_min=0):
  list_captions = []
  for key,val in pic_cap_dict.items():
    for caption in val:
      list_captions.append(caption)

  # Let's consider the words which occured atleast 5 times in the caption
  word_count_dict = {}
  ix_word_dict = {}

  for caption in list_captions:
    for word in caption.split():
      word_count_dict[word] = word_count_dict.get(word,0) + 1

  all_words = [word for word,count in word_count_dict.items() if word_count_min < count]
  return word_count_dict,all_words


# Word to index tokenizer
def word_index(pic_cap_dict):
  list_cap = []
  for key,values in pic_cap_dict.items():
    [list_cap.append(cap) for cap in values]
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(list_cap)
  return tokenizer

def get_list_cap(pic_cap_dict):
  list_cap = []
  for key,values in pic_cap_dict.items():
    [list_cap.append(cap) for cap in values]
  return list_cap

#Get word count
word_count, all_words = get_vocab(pic_cap_dict)

#Get tokenizer object
tok_words = word_index(pic_cap_dict)

#Get list of captions
list_cap = get_list_cap(pic_cap_dict)
vocab_size = len(tok_words.word_index) + 1


print(tok_words.word_index['endseq'])
print("Total words {}".format(vocab_size))
for key,value in pic_cap_dict.items():
    seq = tok_words.texts_to_sequences(list(value))[0]
    print("To caption to index")
    print(seq)
    seq = pad_sequences([seq], maxlen=32, padding='post')[0]
    print("\nFinal padding")
    print(seq)
    break

!ls /content/drive/My\ Drive/dataset/text/Glove\ vectors

# GLoVE extraction

glove_file = open('/content/drive/My Drive/dataset/text/Glove vectors/glove.6B.200d.txt', encoding="utf8")

embeddings_dictionary = dict()
for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions

glove_file.close()

embedding_matrix = zeros((vocab_size, 200))

for word, index in tok_words.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

lena = 0
all_img_ids = []

for key,val in pic_cap_dict.items():
    all_img_ids.append(key)    
    for i in val:
        lena = max(lena,len(i.split()))


vocab_size = len(tok_words.word_index) + 1

print("Number of img ids : {}".format(len(all_img_ids)))
print("Max len sentence {} ".format(lena))
print("Vocabulary size {}".format(vocab_size))

tok_words.index_word[1]

# extract features from each photo in the directory
def extract_features(directory, set_number):
	which_set = [ train_set, test_set, val_set]
	# load the model
	model = InceptionV3(weights='imagenet')
	# re-structure the model
	# model.layers.pop()
	model = Model(inputs=model.inputs, outputs=model.layers[-2].output)
	# summarize
	print(model.summary())
	# extract features from each photo
	img_features_dict = dict()
	i = 1
	for name in listdir(directory):
		# get image id
		image_id = name.split('.')[0]
		if image_id in which_set[set_number]:
			# load an image from file
			filename = directory + '/' + name
			
			# For VGG19
			# image = load_img(filename, target_size=(224, 224))

			# For InceptionV3
			image = load_img(filename, target_size=(299, 299))
   
			# convert the image pixels to a numpy array
			image = img_to_array(image)
			
			# reshape data for the model VGG19
			image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
			
			# prepare the image for the VGG model
			image = preprocess_input(image)
			
			# get features
			feature = model.predict(image, verbose=0)
			
			# store feature
			img_features_dict[image_id] = feature
			# print("{} {}".format(name,feature.shape))
			if i % 1000 == 0 : print(i,)
			i += 1
	return img_features_dict

"""
	Input :
		tokenizer, max length of sentence, pic_cap_dict, img_feat_dict, len(all_words)
	Output :

"""
def create_sequences(tok, max_length, descriptions, photos, vocab_size, train_list):
	feat, cap, out_cap = list(), list(), list()
	# walk through each image identifier in train list
	for key in train_list:
		# walk through each description for the image
		for desc in descriptions[key]:
			# encode the sequence
			seq = tok.texts_to_sequences([desc])[0]
			# split one sequence into multiple X,y pairs
			for i in range(1, len(seq)):
				# split into input and output pair
				in_seq, out_seq = seq[:i], seq[i]
				# pad input sequence
				in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
				# encode output sequence
				out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
				# store
				feat.append(photos[key])
				cap.append(in_seq)
				out_cap.append(out_seq)
	return array(feat), array(cap), array(out_cap)
 
###############################################################################
# FOR LOW RAM
###############################################################################

# data generator, intended to be used in a call to model.fit_generator()
def data_generator(descriptions, photos, tokenizer, max_length, vocab_size, train_list, batch_size = 32):
	# loop for ever over images
	count = 0
	img_feat, img_caps, img_out_caps = list(), list(), list()
	while 1:
		for key in train_list:
			# retrieve the photo feature
			photo = photos[key]
			in_img, in_seq, out_word = create_sequences_generator(tokenizer, max_length, descriptions[key], photo, vocab_size)
			# yield [[in_img, in_seq], out_word]
			for i in range(len(in_img)):
				img_feat.append(in_img[i])
				img_caps.append(in_seq[i])
				img_out_caps.append(out_word[i])
			count += 1
			if count >= batch_size:
				img_feat = asarray(img_feat)
				img_caps = asarray(img_caps)
				img_out_caps = asarray(img_out_caps)
				# print(len(img_feat),len(img_caps),len(img_out_caps))
				yield ([img_feat, img_caps], img_out_caps)
				img_feat, img_caps, img_out_caps = list(), list(), list()
				count = 0


# create sequences of images, input sequences and output words for an image
def create_sequences_generator(tokenizer, max_length, desc_list, photo, vocab_size):
	feat, cap, out_cap = list(), list(), list()
	# walk through each description for the image
	for desc in desc_list:
		# encode the sequence
		seq = tokenizer.texts_to_sequences([desc])[0]
		# split one sequence into multiple X,y pairs
		for i in range(1, len(seq)):
			# split into input and output pair
			in_seq, out_seq = seq[:i], seq[i]
			# pad input sequence
			in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
			# encode output sequence
			out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
			# store
			feat.append(photo[0])
			cap.append(in_seq)
			out_cap.append(out_seq)
	return array(feat), array(cap), array(out_cap)

!ls drive/My\ Drive/dataset/images/

# extract features 
train_img_feat_dict = extract_features(directory, 0)
test_img_feat_dict = extract_features(directory, 1)
val_img_feat_dict = extract_features(directory, 2)

!ls /content/drive/My\ Drive/Features/

# train_feat_file = "drive/My Drive/Colab Notebooks/Project594/Features/features_train_19.pkl"
# test_feat_file = "drive/My Drive/Colab Notebooks/Project594/Features/features_test_19.pkl"
# val_feat_file = "drive/My Drive/Colab Notebooks/Project594/Features/features_val_19.pkl"

train_feat_file = "/content/drive/My Drive/Features/features_train_19.pkl"
test_feat_file = "/content/drive/My Drive/Features/features_test_19.pkl"
val_feat_file = "/content/drive/My Drive/Features/features_val_19.pkl"

# save to file
# dump(train_img_feat_dict, open(train_feat_file, 'wb'))
# dump(val_img_feat_dict, open(val_feat_file, 'wb'))
# dump(test_img_feat_dict, open(test_feat_file, 'wb'))

# Either create features using extract features or load from gdrive
feat_file = open(train_feat_file,'rb')
train_img_feat_dict = load(feat_file)

feat_file = open(val_feat_file,'rb')
val_img_feat_dict = load(feat_file)

feat_file = open(test_feat_file,'rb')
test_img_feat_dict = load(feat_file)

# Create proper input sequence
vocab_size = len(tok_words.word_index) + 1
# train_feat_inp1, train_cap_inp2, train_y = create_sequences(tok_words, 32, pic_cap_dict, train_img_feat_dict, vocab_size, list(train_set))

generator = data_generator(pic_cap_dict, train_img_feat_dict, tok_words, 32, vocab_size, train_list)
inputs, outputs = next(generator)
print(inputs[0].shape)
print(inputs[1].shape)
print(outputs.shape)
len(pic_cap_dict)

# Define the model VGG16
def define_model(vocab_size, max_length):
	# feature extractor model
	inputs1 = Input(shape=(4096,))
	fe1 = Dropout(0.5)(inputs1)
	fe2 = Dense(256, activation='relu')(fe1)
	# sequence model
	inputs2 = Input(shape=(max_length,))
	se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
	# se1 = Embedding(vocab_size,256)(inputs2)
	se2 = Dropout(0.5)(se1)
	se3 = GRU(256)(se2)
	# decoder model
	decoder1 = add([fe2, se3])
	decoder2 = Dense(256, activation='relu')(decoder1)
	outputs = Dense(vocab_size, activation='softmax')(decoder2)
	# tie it together [image, seq] [word]
	model = Model(inputs=[inputs1, inputs2], outputs=outputs)
	model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
	# summarize model
	print(model.summary())
	plot_model(model, to_file='model.png', show_shapes=True)
	return model

# Define the model VGG19
def define_model_2(vocab_size, max_length):
	# feature extractor model
	img_inputs = Input(shape=(4096,))
	fe1 = Dropout(0.5)(img_inputs)
	fe2 = Dense(256, activation='relu')(fe1)
 

	# sequence model
	cap_inputs = Input(shape=(max_length,))
	se1 = Embedding(vocab_size, 256, mask_zero=True)(cap_inputs)
	se2 = Dropout(0.5)(se1)
	se3 = GRU(256)(se2)
 
	# decoder model
	decoder1 = concatenate([fe2, se3])
	decoder2 = Dense(512, activation='relu')(decoder1)
	dropout_dec = Dropout(0.5)(decoder2)
	outputs = Dense(vocab_size, activation='softmax')(dropout_dec)

	# tie it together [image, seq] [word]
	model = Model(inputs=[img_inputs, cap_inputs], outputs=outputs)
	model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
	# summarize model
	print(model.summary())
	plot_model(model, to_file='model.png', show_shapes=True)
	return model

# model = define_model_2(vocab_size, max_length=34)
# # Define the model InceptionV3
def define_model_inc(vocab_size, max_length):
	# feature extractor model
	img_inputs = Input(shape=(2048,))
	fe1 = Dropout(0.5)(img_inputs)
	fe2 = Dense(256, activation='relu')(fe1)
 

	# sequence model
	cap_inputs = Input(shape=(max_length,))
	se1 = Embedding(vocab_size, 200, weights=[embedding_matrix], trainable=False, mask_zero=True)(cap_inputs)
	se2 = Dropout(0.5)(se1)
	se3 = GRU(256)(se2)
 
	# decoder model
	decoder1 = concatenate([fe2, se3])
	decoder2 = Dense(512, activation='relu')(decoder1)
	dropout_dec = Dropout(0.5)(decoder2)
	outputs = Dense(vocab_size, activation='softmax')(dropout_dec)

	# tie it together [image, seq] [word]
	model = Model(inputs=[img_inputs, cap_inputs], outputs=outputs)
	model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
	# summarize model
	print(model.summary())
	plot_model(model, to_file='model.png', show_shapes=True)
	return model

# vocab_size = 7954
# Init the model
model = define_model_inc(vocab_size, max_length=34)

import h5py
# hf = h5py.File('/content/drive/My Drive/Colab Notebooks/Project594/Model Weights 2/model-ep001-loss4.241.h5', 'w')
# !ls /content/drive/My\ Drive/Colab\ Notebooks/Project594/Model\ Weights\ 2
# !ls /content/drive/My\ Drive/Colab\ Notebooks/Project594/Results
# model = load_model('/content/drive/My Drive/Colab Notebooks/Project594/Model Weights 2/model-ep001-loss4.142.h5')
history.history.keys()

project_dir
!ls /content/drive/My\ Drive/project/

# Define the model VGG16
def define_model(vocab_size, max_length):
	# feature extractor model
	inputs1 = Input(shape=(4096,))
	fe1 = Dropout(0.5)(inputs1)
	fe2 = Dense(256, activation='relu')(fe1)
	# sequence model
	inputs2 = Input(shape=(max_length,))
	se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
	# se1 = Embedding(vocab_size,256)(inputs2)
	se2 = Dropout(0.5)(se1)
	se3 = GRU(256)(se2)
	# decoder model
	decoder1 = add([fe2, se3])
	decoder2 = Dense(256, activation='relu')(decoder1)
	outputs = Dense(vocab_size, activation='softmax')(decoder2)
	# tie it together [image, seq] [word]
	model = Model(inputs=[inputs1, inputs2], outputs=outputs)
	model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
	# summarize model
	print(model.summary())
	plot_model(model, to_file='model.png', show_shapes=True)
	return model

# Define the model VGG19
def define_model_2(vocab_size, max_length):
	# feature extractor model
	img_inputs = Input(shape=(4096,))
	fe1 = Dropout(0.5)(img_inputs)
	fe2 = Dense(256, activation='relu')(fe1)
 

	# sequence model
	cap_inputs = Input(shape=(max_length,))
	se1 = Embedding(vocab_size, 256, mask_zero=True)(cap_inputs)
	se2 = Dropout(0.5)(se1)
	se3 = GRU(256)(se2)
 
	# decoder model
	decoder1 = concatenate([fe2, se3])
	decoder2 = Dense(512, activation='relu')(decoder1)
	dropout_dec = Dropout(0.5)(decoder2)
	outputs = Dense(vocab_size, activation='softmax')(dropout_dec)

	# tie it together [image, seq] [word]
	model = Model(inputs=[img_inputs, cap_inputs], outputs=outputs)
	model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
	# summarize model
	print(model.summary())
	plot_model(model, to_file='model.png', show_shapes=True)
	return model

# Define the model InceptionV3
def define_model_inc(vocab_size, max_length):
	# feature extractor model
	img_inputs = Input(shape=(2048,))
	fe1 = Dropout(0.5)(img_inputs)
	fe2 = Dense(256, activation='relu')(fe1)
 

	# sequence model
	cap_inputs = Input(shape=(max_length,))
	# se1 = Embedding(vocab_size, 200, weights=[embedding_matrix], mask_zero=True)(cap_inputs)
	se1 = Embedding(vocab_size, 256, mask_zero=True)(cap_inputs)
	se2 = Dropout(0.5)(se1)
	se3 = GRU(256)(se2)
 
	# decoder model
	decoder1 = concatenate([fe2, se3])
	decoder2 = Dense(512, activation='relu')(decoder1)
	dropout_dec = Dropout(0.5)(decoder2)
	outputs = Dense(vocab_size, activation='softmax')(dropout_dec)

	# tie it together [image, seq] [word]
	model = Model(inputs=[img_inputs, cap_inputs], outputs=outputs)
	model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
	# summarize model
	print(model.summary())
	plot_model(model, to_file='model.png', show_shapes=True)
	return model

# vocab_size = 7954
# Init the model
model = define_model_inc(vocab_size, max_length=34)

# Start Training

# define checkpoint callback
model_num = 11
batch_size = 16
#-acc{acc:.3f}
filepath = project_dir + 'Model '+ '11' + '/model_' + '1' + '-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'
checkpoint = ModelCheckpoint(filepath, monitor='val_loss',verbose=1, save_best_only=True, mode='min')

# model.summary()
epochs = 4
steps = len(train_list)
model = load_model('/content/drive/My Drive/project/Model 11/model_1-ep004-loss4.149-val_loss4.470.h5')
# model = load_model('/content/drive/My Drive/Colab Notebooks/Project594/Model 11/model_256-ep004-loss3.306-val_loss3.781-acc0.295.h5')
train_generator = data_generator(pic_cap_dict, train_img_feat_dict, tok_words, 34, vocab_size, train_list, batch_size)
val_generator   = data_generator(pic_cap_dict, val_img_feat_dict, tok_words, 34, vocab_size, val_list, batch_size)
# fit for 20 epochs  -- 
history = model.fit_generator(train_generator, callbacks=[checkpoint],epochs=epochs, steps_per_epoch=steps//batch_size,validation_data=val_generator, 
                              validation_steps=len(val_list)//batch_size, verbose=1,  use_multiprocessing=True,
                              workers=8, max_queue_size=128, shuffle=True)

dump(history.history, open(project_dir+"Results/model_{}_512_history.pkl".format(model_num), 'wb'))

!ls /content/drive/My\ Drive/project/

# generate a description for an image

def generate_desc(model, tokenizer, photo, max_length):
	# seed the generation process
	in_text = 'startseq'
	# iterate over the whole length of the sequence
	for i in range(max_length):
		# integer encode input sequence
		sequence = tokenizer.texts_to_sequences([in_text])[0]
		# pad input
		sequence = pad_sequences([sequence], maxlen=max_length)
		# predict next word
		y_pred = model.predict([photo,sequence], verbose=0)
		# convert probability to integer
		y_pred = argmax(y_pred)
		# map integer to word
		word = tokenizer.index_word[y_pred] if y_pred in tokenizer.index_word else None
		# stop if we cannot map the word
		if word is None:
			break
		# append as input for generating the next word
		in_text += ' ' + word
		# stop if we predict the end of the sequence
		if word == 'endseq':
			break
	return in_text

# evaluate the skill of the model
def evaluate_model(model, descriptions, photos, tokenizer, max_length, val_list):
	actual, predicted = list(), list()
	# step over the whole set
	count = 1
	for key in val_list:
		# generate description

		desc_list = descriptions[key]
		y_pred_words = generate_desc(model, tokenizer, photos[key], max_length)
		# store actual and predicted
		references = [d.split() for d in desc_list]
		actual.append(references)
		predicted.append(y_pred_words.split())
		if count % 100 == 0: print(count)
		count += 1
		
	# calculate BLEU score
	b_1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))
	b_2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))
	b_3 = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))
	b_4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))
	
	return [b_1,b_2,b_3,b_4]

   
# start = time.time()
# model = load_model("/content/drive/My Drive/Colab Notebooks/Project594/Model 10/model_14_512-ep004-loss2.799-val_loss3.626-acc0.362.h5")
# # model = load_model("/content/drive/My Drive/Colab Notebooks/Project594/Model 11/model_256-ep008-loss3.273-val_loss3.782-acc0.317.h5")
# # model = load_model("/content/drive/My Drive/Colab Notebooks/Project594/Model 11/model_256-ep007-loss3.160-val_loss3.793-acc0.330.h5")
# bleu_list = evaluate_model(model, pic_cap_dict, val_img_feat_dict, tok_words, 34, val_list)
# generator_val = data_generator(pic_cap_dict, val_img_feat_dict, tok_words, 34, vocab_size, val_list)

# print('BLEU-1: {}'.format(bleu_list[0]))
# print('BLEU-2: {}'.format(bleu_list[1]))
# print('BLEU-3: {}'.format(bleu_list[2]))
# print('BLEU-4: {}'.format(bleu_list[3]))

# eval_mod		= model.evaluate_generator(generator = generator_val, steps=len(val_list),verbose=0, use_multiprocessing=True, workers=4)
# end = time.time()

# print(eval_mod)
# print("{}th model done ---- time elapsed = {}".format(19, (end - start)))
# print("\n ########################################### \n")	
# results_dict = open(project_dir+'Results/first_model.pkl', 'wb')
# results_dict['model_{}'.format(19)] = [bleu_list,eval]
# dump(results_dict, open(project_dir+'Results/first_model.pkl', 'wb'))


# for i in range(19):
# 	model = load_model(project_dir + "Model weights/model_{}.h5".format(i))
# 	start = time.time()
# 	bleu_list = evaluate_model(model, pic_cap_dict, val_img_feat_dict, tok_words, 32, val_list)
# 	generator_val = data_generator(pic_cap_dict, val_img_feat_dict, tok_words, 32, vocab_size, val_list)
# 	eval_mod		= model.evaluate_generator(generator = generator_val, verbose=0, use_multiprocessing=True, workers=2)
# 	end = time.time()
# 	print('BLEU-1: {}'.format(belu_list[0]))
# 	print('BLEU-2: {}'.format(belu_list[1]))
# 	print('BLEU-3: {}'.format(belu_list[2]))
# 	print('BLEU-4: {}'.format(belu_list[3]))
# 	print("{}th model done ---- time elapsed = {}".format(i, (end - start)))
# 	print("\n ########################################### \n")	
# 	results_dict = open(project_dir+'Results/first_model.pkl', 'wb')
# 	results_dict['model_{}'.format(i)] = [bleu_list,eval]
# 	dump(results_dict, open(project_dir+'Results/first_model.pkl', 'wb'))

# eval_mod		= model.evaluate_generator(generator = generator_val, steps=1000,verbose=1, use_multiprocessing=True, workers=2)
print('BLEU-1: {}'.format(bleu_list[0]))
print('BLEU-2: {}'.format(bleu_list[1]))
print('BLEU-3: {}'.format(bleu_list[2]))
print('BLEU-4: {}'.format(bleu_list[3]))
print("{}th model done ---- time elapsed = {}".format(19, (end - start)))
print("\n ########################################### \n")	
results_dict = open(project_dir+'Results/first_model.pkl', 'wb')
results_dict['model_{}'.format(19)] = [bleu_list,eval]
dump(results_dict, open(project_dir+'Results/first_model.pkl', 'wb'))

import seaborn as sns

sns.set()

# res_dict
res_dict = None
with open(project_dir+'Results/model_10_64_history.pkl', 'rb') as f:
    res_dict = load(f)

with open(project_dir+'Results/model_10_14_128_history.pkl', 'rb') as f:
    res_dict_2 = load(f)

with open(project_dir+'Results/model_10_14_256_history.pkl', 'rb') as f:
    res_dict_3 = load(f)

with open(project_dir+'Results/model_10_14_512_history.pkl', 'rb') as f:
    res_dict_4 = load(f)

print(res_dict)
print(res_dict_2)
print(res_dict_3)
print(res_dict_4)
count = 13

# with open(project_dir+'Results/model_8_2_history.pkl', 'rb') as f:
#     len_res = len(res_dict)
#     res_dict_2 = load(f)
#     for key, val in res_dict_2.items():
#         res_dict[key].extend(val)

# with open(project_dir+'Results/model_9_128_history.pkl', 'rb') as f:
#     len_res = len(res_dict)
#     res_dict_2 = load(f)
#     for key, val in res_dict_2.items():
#         res_dict[key].extend(val)

# print(res_dict)
import matplotlib.pyplot as plt


plt.plot(range(1,15), res_dict['loss'][:14], color='midnightblue', label = 'Train loss 64')
plt.plot(range(1,15), res_dict['val_loss'][:14], color='darkred', label = 'Val loss 64')
# plt.plot(range(1,15), res_dict['acc'][:14], color='orange')
# plt.plot(range(1,15), res_dict['val_acc'][:14], color='pink')

plt.plot(range(15,21), res_dict_2['loss'][:6], linestyle='--',color='mediumblue', label = 'Train loss 128')
plt.plot(range(15,21), res_dict_2['val_loss'][:6], linestyle='--',color='orangered', label = 'Val loss 128')
# plt.plot(range(14,20), res_dict_2['acc'][:6], color='orange')
# plt.plot(range(14,20), res_dict_2['val_acc'][:6], color='pink')

plt.plot(range(21,24), res_dict_3['loss'][:3], linestyle='-.',color='royalblue', label = 'Train loss 256')
plt.plot(range(21,24), res_dict_3['val_loss'][:3], linestyle='-.',color='red', label = 'Val loss 256')
# plt.plot(range(19,22), res_dict_3['acc'][:3], color='orange')
# plt.plot(range(19,22), res_dict_3['val_acc'][:3], color='pink')

plt.legend(frameon=True)
plt.rcParams["figure.figsize"] = (10,10)
plt.grid(b=True, which='major', color='#666666', linestyle='-')

# Show the minor grid lines with very faint and almost transparent grey lines
plt.minorticks_on()
plt.grid(b=True, which='minor', color='#999990', linestyle='-', alpha=0.2)
plt.xlabel('Number of epochs', fontsize=18)
plt.ylabel('Loss Values', fontsize=16)
# plt.xticks(np.arange(0,25))


# plt.plot(range(len(res_dict['acc'])), res_dict['loss'], color='blue')
# plt.plot(range(len(res_dict['acc'])), res_dict['val_loss'], color='red')
# plt.plot(range(len(res_dict['acc'])), res_dict['acc'], color='orange')
# plt.plot(range(len(res_dict['acc'])), res_dict['val_acc'], color='pink')

plot_model(model, to_file='model.png', show_shapes=True)

model = load_model("/content/drive/My Drive/Colab Notebooks/Project594/Model 10/model_14_512-ep004-loss2.799-val_loss3.626-acc0.362.h5")
# model = load_model("/content/drive/My Drive/Colab Notebooks/Project594/Model 11/model_64-ep001-loss3.160-val_loss3.845-acc0.321.h5")

for key,val in test_img_feat_dict.items():
    img = mpimg.imread(directory+''+key+'.jpg')
    imgplot = plt.imshow(img)
    plt.show()
    test_out = generate_desc(model, x, val, 34)
    print(test_out)
    print(pic_cap_dict[key])
    input()

import matplotlib.pyplot as plt

# this only gives the training loss
loss_values = hist.history['loss']
epochs = range(1, len(loss_values)+1)
# For checking validation loss, the validation_split parameter must be included in model.fit()
# Example
"""
history = model.fit(train_exp,epochs=epochs, steps_per_epoch=steps//batch_size, 
                              
                              
                              
                           --->   validation_split=0.33  <----
                                    Include this attribute in the fit function call
                              
                              
                              
                              , verbose=1, 
                              use_multiprocessing=True, workers=8, max_queue_size=128, shuffle=True)

"""

loss_values = hist.history['val_loss']
epochs = range(1, len(loss_values)+1)

plt.plot(epochs, loss_values, label='Training Loss')
plt.xlabel('Epochs')

plt.plot(epochs, loss_values, label='Test Loss')
plt.xlabel('Epochs')
plt.plot(epochs, loss_values, label='Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss Values')

plt.show()

def imgfeature(filename):
    img_features_dict = dict()
    model = InceptionV3(weights='imagenet')
    # re-structure the model
    # model.layers.pop()
    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)
    image = load_img(filename, target_size=(299, 299))
    # convert the image pixels to a numpy array
    image = img_to_array(image)
    # reshape data for the model VGG19
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
    # prepare the image for the VGG model
    image = preprocess_input(image)
    # get features
    feature = model.predict(image, verbose=0)
    # store feature
    img_features_dict[1] = feature
    return img_features_dict

directory = '/content/drive/My Drive/test/10815824_2997e03d76.jpg'
model = load_model("/content/drive/My Drive/project/Model 11/model_1-ep004-loss4.149-val_loss4.470.h5")
img_dic = imgfeature(directory)
for key,val in img_dic.items():
    img = mpimg.imread(directory)
    imgplot = plt.imshow(img)
    plt.show()
    test_out = generate_desc(model, tok_words, val, 34)
    print(test_out)
    # print(pic_cap_dict[key])
    input()